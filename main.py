# -*- coding: utf-8 -*-
"""solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h_yt4S4a6c5l3e9u-rMyDdAnuZfB42il
"""

import pandas as pd

abt = pd.read_csv('abt.csv')
buy = pd.read_parquet('buy.csv')
xref = pd.read_parquet('xref.csv')

print(xref.head())  # Change the print statement to explore more

import recordlinkage.preprocessing as rlp

abt['price'] = abt['price'].str.replace('[^0-9.]', '', regex=True).astype(float)
abt['manufacturer'] = rlp.clean(abt['name'].apply(lambda s: s.split(' ')[0]))
abt['product_name'] = rlp.clean(abt['name'].apply(lambda s: ' '.join(s.split(' ')[1:])))
abt['product_code'] = rlp.clean((abt['name'].fillna('') + abt.description.fillna('')).str.extract('([0-9A-Z\\-]{5,})')[0])

buy['price'] = buy.price.str.replace('[^0-9.]', '', regex=True).astype(float)
buy['manufacturer'] = rlp.clean(buy.name.str.lower().apply(lambda s: s.split(' ')[0]))
buy['product_name'] = rlp.clean(buy.name.str.lower().apply(lambda s: ' '.join(s.split(' ')[1:])))
buy['product_code'] = rlp.clean((buy.name.fillna('') + buy.description.fillna('')).str.extract('([0-9A-Z\\-]{5,})')[0])

import recordlinkage as rl

indexer = rl.Index()
indexer.block('manufacturer')
candidate_pairs = indexer.index(abt, buy)

actual_matches = pd.MultiIndex.from_frame(xref)

# Evaluate recall:
n_true_positives = actual_matches.intersection(candidate_pairs).shape[0]
n_false_negatives = actual_matches.difference(candidate_pairs).shape[0]

num_candidates = candidate_pairs.shape[0]
num_brute_force = abt.shape[0] * buy.shape[0]
print('Index size: ', num_candidates)
print('Compression factor: ', num_candidates / num_brute_force)
print('Recall: ', n_true_positives / (n_true_positives + n_false_negatives))

import lancedb
from lancedb.embeddings import with_embeddings
from sentence_transformers import SentenceTransformer

# Load a pretrained model:
model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')

# Connect to the lancedb instance:
db = lancedb.connect('./project_lance.lancedb')

# Add embeddings and store in lance format:
def embed_func(batch):
    return [model.encode(sentence) for sentence in batch]

data_abt = with_embeddings(embed_func, abt.reset_index(), column='name', show_progress=True)
db.create_table('abt', data_abt, mode='overwrite')

data_buy = with_embeddings(embed_func, buy.reset_index(), column='name', show_progress=True)
db.create_table('buy', data_buy, mode='overwrite')

# Build the index:
k = 10
res = []

tbl_abt = db.open_table('abt')
tbl_buy = db.open_table('buy')

for _, row in tbl_abt.to_pandas().iterrows():
    df_response = tbl_buy.search(row['vector']).metric('cosine').limit(k).to_pandas()
    res.append(df_response[['id_buy', '_distance']].assign(id_abt=row['id_abt']))  # also store distance as meta data

# Reshape search results to meet the format of the ground truth:
res = pd.concat(res, ignore_index=True)
res = res[['id_abt', 'id_buy', '_distance']]

candidate_links = pd.MultiIndex.from_frame(res[['id_abt', 'id_buy']])

# Evaluate the index:
actual_matches = pd.MultiIndex.from_frame(xref)

# Evaluate recall:
n_true_positives = actual_matches.intersection(candidate_links).shape[0]
n_false_negatives = actual_matches.difference(candidate_links).shape[0]

num_candidates = candidate_links.shape[0]
num_brute_force = abt.shape[0] * buy.shape[0]
print('Index size: ', num_candidates)
print('Compression factor: ', num_candidates / num_brute_force)
print('Recall: ', n_true_positives / (n_true_positives + n_false_negatives))

import recordlinkage as rl

comparer = rl.Compare()
comparer.string(left_on='product_name', right_on='product_name', method='jaro_winkler', label='name_score_jw')
comparer.string(left_on='product_name', right_on='product_name', method='lcs', label='name_score_lcs')
comparer.string(left_on='product_code', right_on='product_code', method='damerau_levenshtein', label='code_score')
comparer.string(left_on='manufacturer', right_on='manufacturer', method='jaro_winkler', label='manufacturer_score')
comparer.numeric(left_on='price', right_on='price', scale=100, label='price_score', missing_value=None)

scores = comparer.compute(candidate_links, abt, buy)

from sklearn.model_selection import train_test_split

# SBERT scores:
sbert_scores = res.set_index(['id_abt', 'id_buy'])._distance.apply(lambda d: 1 / (1 + d)).rename('sbert_score')
scores = pd.concat([scores, sbert_scores], axis=1)

# Add label:
actual_matches = pd.MultiIndex.from_frame(xref)
scores['class'] = 0.
scores.loc[scores.index.isin(actual_matches), 'class'] = 1.

# Split:
df_train, df_test = train_test_split(scores, test_size=0.5, stratify=scores['class'], random_state=1)

from catboost import CatBoostClassifier
from sklearn.metrics import precision_score, recall_score, f1_score

model = CatBoostClassifier(random_state=1)

model_features = scores.columns[:-1]
model.fit(X=df_train[model_features], y=df_train['class'], verbose=False)

y_test_pred = model.predict(df_test[model_features])
print('---\nPrecision on test set: ', precision_score(df_test['class'], y_test_pred))
print('Recall on test set: ', recall_score(df_test['class'], y_test_pred))
print('F1 on test set: ', f1_score(df_test['class'], y_test_pred))

import numpy as np
from cleanlab.multiannotator import get_active_learning_scores

y_test_pred = model.predict_proba(df_test[model_features])

# 1. Estimate informativeness of predictions in test set:
_, y_test_al_scores = get_active_learning_scores(df_train['class'], pred_probs_unlabeled=y_test_pred)

# Identify position of least informative 500 scores (increase train sample by roughly 10%):
idx_next_batch = np.argsort(y_test_al_scores)[:500]
df_train_next = df_test.iloc[idx_next_batch]

# 2. Add least informative 100 to training after simulated hand-labeling:
df_train_new = pd.concat([df_train, df_train_next])
df_test_new = df_test.loc[~df_test.index.isin(df_train_next.index)]

model = CatBoostClassifier(random_state=1)

model_features = scores.columns[:-1]
model.fit(X=df_train_new[model_features], y=df_train_new['class'], verbose=False)

y_test_pred_new = model.predict(df_test_new[model_features])
print('---\nPrecision on test set: ', precision_score(df_test_new['class'], y_test_pred_new))
print('Recall on test set: ', recall_score(df_test_new['class'], y_test_pred_new))
print('F1 on test set: ', f1_score(df_test_new['class'], y_test_pred_new))

import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer(df_test[model_features], df_test['class'])

df_importance = pd.DataFrame({
    'importance': np.abs(shap_values.values).mean(axis=0),
    'feature': model_features
}).sort_values('importance', ascending=False)

print(df_importance)

shap.dependence_plot('code_score', shap_values.values, df_test[model_features])































